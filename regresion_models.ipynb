{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all of the necessary packages for muliple classifiers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.preprocessing import scale \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV, LinearRegression, ARDRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm.classes import OneClassSVM\n",
    "from sklearn.neighbors.classification import RadiusNeighborsClassifier\n",
    "from sklearn.neighbors.classification import KNeighborsClassifier\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model.stochastic_gradient import SGDClassifier\n",
    "from sklearn.linear_model.ridge import RidgeClassifierCV\n",
    "from sklearn.linear_model.ridge import RidgeClassifier\n",
    "from sklearn.linear_model.passive_aggressive import PassiveAggressiveClassifier    \n",
    "from sklearn.gaussian_process.gpc import GaussianProcessClassifier\n",
    "from sklearn.ensemble.weight_boosting import AdaBoostClassifier\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.ensemble.bagging import BaggingClassifier\n",
    "from sklearn.ensemble.forest import ExtraTreesClassifier\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.naive_bayes import MultinomialNB  \n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.utils.testing import all_estimators\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import decomposition\n",
    "import xgboost as xgb\n",
    "# Fixing random state for reproducibility\n",
    "np.random.seed(19680801)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify all the files we want to process in a list\n",
    "all_genes = [\"all_fly_genes.csv\", \"all_human_genes.csv\"]\n",
    "forty_one_genes = [\"41_fly_human.csv\", \"41_human_fly.csv\"]\n",
    "aging_cor_genes = [\"1000_fly.csv\", \"1000_human.csv\" ]\n",
    "homologous_genes = [\"Transposed_back_othologous_genes_combined_species_zeros_dropped_fly.csv\", \"Transposed_back_othologous_genes_combined_species_zeros_dropped_human.csv\"]\n",
    "overlapping_genes = ['all_flies_human_correlated_genes.csv', 'all_human_fly_correlations.csv']\n"
    "# human_aging_correlated_top_1000 --- human top 1000 aging correlated genes\n",
    "# all_human_fly_correlations.csv --- huamn top 800 something genes based on fly aging correlations\n",
    "# all_human_fly_overlap --- human top 41 overlapping genes\n",
    "# all_fly_human_overlap --- fly top 41 overlapping genes\n",
    "# flies_aging_top_1000 --- all top 1000 fly aging genes\n",
    "# flies_257_human_aging_correlated_genes.csv --- flies 257 human aging correlated genes or something like that\n",
    "# all_fly_genes --- all fly genes\n",
    "#all_human_genes --- all_human_genes\n",
    "# homologs fly genes... --- transposed_back...fly\n",
    "# homologs. human genes.. --- transposed_back...human\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify a list of all regression models to test out\n",
    "regression_models = [LogisticRegression(), LinearRegression, ARDRegression(), MultinomialNB(), Lasso(), Ridge(), SVC(),\\\n",
    "                    GradientBoostingRegressor(), RandomForestRegressor(), ExtraTreesRegressor(), xgb.XGBRegressor(),\\\n",
    "                    LinearDiscriminantAnalysis(), sklearn.linear_model.HuberRegressor(),sklearn.ensemble.AdaBoostRegressor()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify a list of all classification models\n",
    "classifiers_to_test = [xgb.XGBClassifier(),ExtraTreeClassifier(), DecisionTreeClassifier(),\\\n",
    "                       RadiusNeighborsClassifier(), KNeighborsClassifier(),SGDClassifier(),\\\n",
    "                       RidgeClassifier(), PassiveAggressiveClassifier(), GaussianProcessClassifier(),\\\n",
    "                       AdaBoostClassifier(), GradientBoostingClassifier(), BaggingClassifier(),\\\n",
    "                       ExtraTreesClassifier(), RandomForestClassifier(), LogisticRegression()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on all of the data and testing on all of the data\n",
      "51"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-c140655c8a88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mmodel_for_use\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregression_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;31m#fit the model on the training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[0mmodel_for_use\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m             \u001b[1;31m#predict the ages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mpredicted_ages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_for_use\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1548\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1549\u001b[1;33m                 sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m   1550\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1551\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[1;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[0;32m    919\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 921\u001b[1;33m         epsilon, sample_weight)\n\u001b[0m\u001b[0;32m    922\u001b[0m     \u001b[1;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m     \u001b[1;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#all_genes regression\n",
    "\n",
    "print(\"Training on all of the data and testing on all of the data\")\n",
    "#load all data\n",
    "import numpy\n",
    "import statistics\n",
    "#create individual datasets of each species\n",
    "for file in all_genes:\n",
    "    for regression_model in regression_models:\n",
    "        raw_df = pd.read_csv(file)\n",
    "        #encode it to integers for use in the classification\n",
    "        raw_df_dropped = raw_df.drop(columns=['Individual',\"Species\",\"Age\",\"Group\"])\n",
    "        #set our Y predictor\n",
    "        ages = raw_df['Age'].astype(int)\n",
    "\n",
    "        # drop these columns from the dataframe\n",
    "\n",
    "\n",
    "        #define lists to hold our results from each algorithim\n",
    "        r_square = list()\n",
    "        median_score = list()\n",
    "        mean_score = list()\n",
    "        mean_squares = list()\n",
    "\n",
    "\n",
    "        # run bootstrap on all data and then tested on all data for the samples\n",
    "        n_iterations = 1000\n",
    "        for i in range(n_iterations):\n",
    "            print('\\r' + str(i),end = '')\n",
    "\n",
    "\n",
    "            #define the data with a new slice of the data\n",
    "            train_x, test_x, train_y, test_y = train_test_split(raw_df_dropped, ages, test_size=0.25, random_state=i)\n",
    "\n",
    "            model_for_use = regression_model\n",
    "            #fit the model on the training data\n",
    "            model_for_use.fit(train_x, train_y)\n",
    "            #predict the ages\n",
    "            predicted_ages = model_for_use.predict(test_x)\n",
    "            #append the scores to lists to report mean values\n",
    "            r_square.append(sklearn.metrics.r2_score(test_y, predicted_ages))\n",
    "            mean_squares.append(mean_squared_error(test_y, predicted_ages))\n",
    "            median_score.append(sklearn.metrics.median_absolute_error(test_y, predicted_ages))\n",
    "            mean_score.append(sklearn.metrics.mean_absolute_error(test_y, predicted_ages))\n",
    "\n",
    "        # #predict the ages\n",
    "        # predicted_ages = model_for_use.predict(test_x)\n",
    "        # a, b = best_fit(predicted_ages, test_y)\n",
    "        # yfit = [a + b * xi for xi in predicted_ages]\n",
    "        # matplotlib.pyplot.scatter(predicted_ages, test_y)\n",
    "        # matplotlib.pyplot.plot(predicted_ages, yfit)\n",
    "        # matplotlib.pyplot.xlabel(\"Predicted Ages in Years\")\n",
    "        # matplotlib.pyplot.ylabel(\"Actual Ages in Years\")\n",
    "        # matplotlib.pyplot.suptitle(\"Human Top 10 Genes Logistic Regression\", size=24)\n",
    "        \n",
    "        print(\"{} using {} \".format(file, regression_model))\n",
    "        print(\"\\nThe Average Mean absolute error is : \",statistics.mean(mean_squares))\n",
    "        print(\"The Average Median absolute error is : \",statistics.mean(median_score))\n",
    "        print(\"The median R squared is : \",statistics.median(r_square))\n",
    "        print(\"The mean R squared is : \",statistics.mean(r_square))\n",
    "\n",
    "        alpha = 0.95\n",
    "        p = ((1.0-alpha)/2.0) * 100\n",
    "        lower = max(0.0, numpy.percentile(r_square, p))\n",
    "        p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "        upper = min(1.0, numpy.percentile(r_square, p))\n",
    "        print('\\n%.1f R^2 confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))\n",
    "        histo = numpy.histogram(r_square)\n",
    "        _ = plt.hist(r_square, bins='auto')\n",
    "        plt.title(\"Histogram of R^2 {} using {} \".format(file, regression_model))\n",
    "        plt.xlim(0,1)\n",
    "        plt.xticks(np.arange(0, 1.1, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forty_one_genes regression\n",
    "\n",
    "print(\"Training on all of the data and testing on all of the data\")\n",
    "#load all data\n",
    "import numpy\n",
    "import statistics\n",
    "#create individual datasets of each species\n",
    "for file in forty_one_genes:\n",
    "    for regression_model in regression_models:\n",
    "        raw_df = pd.read_csv(file)\n",
    "        #encode it to integers for use in the classification\n",
    "        raw_df_dropped = raw_df.drop(columns=['Individual',\"Species\",\"Age\",\"Group\"])\n",
    "        #set our Y predictor\n",
    "        ages = raw_df['Age'].astype(int)\n",
    "\n",
    "        # drop these columns from the dataframe\n",
    "\n",
    "\n",
    "        #define lists to hold our results from each algorithim\n",
    "        r_square = list()\n",
    "        median_score = list()\n",
    "        mean_score = list()\n",
    "        mean_squares = list()\n",
    "\n",
    "\n",
    "        # run bootstrap on all data and then tested on all data for the samples\n",
    "        n_iterations = 1000\n",
    "        for i in range(n_iterations):\n",
    "            print('\\r' + str(i),end = '')\n",
    "\n",
    "\n",
    "            #define the data with a new slice of the data\n",
    "            train_x, test_x, train_y, test_y = train_test_split(raw_df_dropped, ages, test_size=0.25, random_state=i)\n",
    "\n",
    "            model_for_use = regression_model\n",
    "            #fit the model on the training data\n",
    "            model_for_use.fit(train_x, train_y)\n",
    "            #predict the ages\n",
    "            predicted_ages = model_for_use.predict(test_x)\n",
    "            #append the scores to lists to report mean values\n",
    "            r_square.append(sklearn.metrics.r2_score(test_y, predicted_ages))\n",
    "            mean_squares.append(mean_squared_error(test_y, predicted_ages))\n",
    "            median_score.append(sklearn.metrics.median_absolute_error(test_y, predicted_ages))\n",
    "            mean_score.append(sklearn.metrics.mean_absolute_error(test_y, predicted_ages))\n",
    "\n",
    "        # #predict the ages\n",
    "        # predicted_ages = model_for_use.predict(test_x)\n",
    "        # a, b = best_fit(predicted_ages, test_y)\n",
    "        # yfit = [a + b * xi for xi in predicted_ages]\n",
    "        # matplotlib.pyplot.scatter(predicted_ages, test_y)\n",
    "        # matplotlib.pyplot.plot(predicted_ages, yfit)\n",
    "        # matplotlib.pyplot.xlabel(\"Predicted Ages in Years\")\n",
    "        # matplotlib.pyplot.ylabel(\"Actual Ages in Years\")\n",
    "        # matplotlib.pyplot.suptitle(\"Human Top 10 Genes Logistic Regression\", size=24)\n",
    "        \n",
    "        print(\"{} using {} \".format(file, regression_model))\n",
    "        print(\"\\nThe Average Mean absolute error is : \",statistics.mean(mean_squares))\n",
    "        print(\"The Average Median absolute error is : \",statistics.mean(median_score))\n",
    "        print(\"The median R squared is : \",statistics.median(r_square))\n",
    "        print(\"The mean R squared is : \",statistics.mean(r_square))\n",
    "\n",
    "        alpha = 0.95\n",
    "        p = ((1.0-alpha)/2.0) * 100\n",
    "        lower = max(0.0, numpy.percentile(r_square, p))\n",
    "        p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "        upper = min(1.0, numpy.percentile(r_square, p))\n",
    "        print('\\n%.1f R^2 confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))\n",
    "        histo = numpy.histogram(r_square)\n",
    "        _ = plt.hist(r_square, bins='auto')\n",
    "        plt.title(\"Histogram of R^2 {} using {} \".format(file, regression_model))\n",
    "        plt.xlim(0,1)\n",
    "        plt.xticks(np.arange(0, 1.1, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aging_cor_genes regression\n",
    "\n",
    "print(\"Training on all of the data and testing on all of the data\")\n",
    "#load all data\n",
    "import numpy\n",
    "import statistics\n",
    "#create individual datasets of each species\n",
    "for file in aging_cor_genes:\n",
    "    for regression_model in regression_models:\n",
    "        raw_df = pd.read_csv(file)\n",
    "        #encode it to integers for use in the classification\n",
    "        raw_df_dropped = raw_df.drop(columns=['Individual',\"Species\",\"Age\",\"Group\"])\n",
    "        #set our Y predictor\n",
    "        ages = raw_df['Age'].astype(int)\n",
    "\n",
    "        # drop these columns from the dataframe\n",
    "\n",
    "\n",
    "        #define lists to hold our results from each algorithim\n",
    "        r_square = list()\n",
    "        median_score = list()\n",
    "        mean_score = list()\n",
    "        mean_squares = list()\n",
    "\n",
    "\n",
    "        # run bootstrap on all data and then tested on all data for the samples\n",
    "        n_iterations = 1000\n",
    "        for i in range(n_iterations):\n",
    "            print('\\r' + str(i),end = '')\n",
    "\n",
    "\n",
    "            #define the data with a new slice of the data\n",
    "            train_x, test_x, train_y, test_y = train_test_split(raw_df_dropped, ages, test_size=0.25, random_state=i)\n",
    "\n",
    "            model_for_use = regression_model\n",
    "            #fit the model on the training data\n",
    "            model_for_use.fit(train_x, train_y)\n",
    "            #predict the ages\n",
    "            predicted_ages = model_for_use.predict(test_x)\n",
    "            #append the scores to lists to report mean values\n",
    "            r_square.append(sklearn.metrics.r2_score(test_y, predicted_ages))\n",
    "            mean_squares.append(mean_squared_error(test_y, predicted_ages))\n",
    "            median_score.append(sklearn.metrics.median_absolute_error(test_y, predicted_ages))\n",
    "            mean_score.append(sklearn.metrics.mean_absolute_error(test_y, predicted_ages))\n",
    "\n",
    "        # #predict the ages\n",
    "        # predicted_ages = model_for_use.predict(test_x)\n",
    "        # a, b = best_fit(predicted_ages, test_y)\n",
    "        # yfit = [a + b * xi for xi in predicted_ages]\n",
    "        # matplotlib.pyplot.scatter(predicted_ages, test_y)\n",
    "        # matplotlib.pyplot.plot(predicted_ages, yfit)\n",
    "        # matplotlib.pyplot.xlabel(\"Predicted Ages in Years\")\n",
    "        # matplotlib.pyplot.ylabel(\"Actual Ages in Years\")\n",
    "        # matplotlib.pyplot.suptitle(\"Human Top 10 Genes Logistic Regression\", size=24)\n",
    "        \n",
    "        print(\"{} using {} \".format(file, regression_model))\n",
    "        print(\"\\nThe Average Mean absolute error is : \",statistics.mean(mean_squares))\n",
    "        print(\"The Average Median absolute error is : \",statistics.mean(median_score))\n",
    "        print(\"The median R squared is : \",statistics.median(r_square))\n",
    "        print(\"The mean R squared is : \",statistics.mean(r_square))\n",
    "\n",
    "        alpha = 0.95\n",
    "        p = ((1.0-alpha)/2.0) * 100\n",
    "        lower = max(0.0, numpy.percentile(r_square, p))\n",
    "        p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "        upper = min(1.0, numpy.percentile(r_square, p))\n",
    "        print('\\n%.1f R^2 confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))\n",
    "        histo = numpy.histogram(r_square)\n",
    "        _ = plt.hist(r_square, bins='auto')\n",
    "        plt.title(\"Histogram of R^2 {} using {} \".format(file, regression_model))\n",
    "        plt.xlim(0,1)\n",
    "        plt.xticks(np.arange(0, 1.1, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homologous_genes\n",
    "#homologous_genes regression\n",
    "\n",
    "print(\"Training on all of the data and testing on all of the data\")\n",
    "#load all data\n",
    "import numpy\n",
    "import statistics\n",
    "#create individual datasets of each species\n",
    "for file in homologous_genes:\n",
    "    for regression_model in regression_models:\n",
    "        raw_df = pd.read_csv(file)\n",
    "        #encode it to integers for use in the classification\n",
    "        raw_df_dropped = raw_df.drop(columns=['Individual',\"Species\",\"Age\",\"Group\"])\n",
    "        #set our Y predictor\n",
    "        ages = raw_df['Age'].astype(int)\n",
    "\n",
    "        # drop these columns from the dataframe\n",
    "\n",
    "\n",
    "        #define lists to hold our results from each algorithim\n",
    "        r_square = list()\n",
    "        median_score = list()\n",
    "        mean_score = list()\n",
    "        mean_squares = list()\n",
    "\n",
    "\n",
    "        # run bootstrap on all data and then tested on all data for the samples\n",
    "        n_iterations = 1000\n",
    "        for i in range(n_iterations):\n",
    "            print('\\r' + str(i),end = '')\n",
    "\n",
    "\n",
    "            #define the data with a new slice of the data\n",
    "            train_x, test_x, train_y, test_y = train_test_split(raw_df_dropped, ages, test_size=0.25, random_state=i)\n",
    "\n",
    "            model_for_use = regression_model\n",
    "            #fit the model on the training data\n",
    "            model_for_use.fit(train_x, train_y)\n",
    "            #predict the ages\n",
    "            predicted_ages = model_for_use.predict(test_x)\n",
    "            #append the scores to lists to report mean values\n",
    "            r_square.append(sklearn.metrics.r2_score(test_y, predicted_ages))\n",
    "            mean_squares.append(mean_squared_error(test_y, predicted_ages))\n",
    "            median_score.append(sklearn.metrics.median_absolute_error(test_y, predicted_ages))\n",
    "            mean_score.append(sklearn.metrics.mean_absolute_error(test_y, predicted_ages))\n",
    "\n",
    "        # #predict the ages\n",
    "        # predicted_ages = model_for_use.predict(test_x)\n",
    "        # a, b = best_fit(predicted_ages, test_y)\n",
    "        # yfit = [a + b * xi for xi in predicted_ages]\n",
    "        # matplotlib.pyplot.scatter(predicted_ages, test_y)\n",
    "        # matplotlib.pyplot.plot(predicted_ages, yfit)\n",
    "        # matplotlib.pyplot.xlabel(\"Predicted Ages in Years\")\n",
    "        # matplotlib.pyplot.ylabel(\"Actual Ages in Years\")\n",
    "        # matplotlib.pyplot.suptitle(\"Human Top 10 Genes Logistic Regression\", size=24)\n",
    "        \n",
    "        print(\"{} using {} \".format(file, regression_model))\n",
    "        print(\"\\nThe Average Mean absolute error is : \",statistics.mean(mean_squares))\n",
    "        print(\"The Average Median absolute error is : \",statistics.mean(median_score))\n",
    "        print(\"The median R squared is : \",statistics.median(r_square))\n",
    "        print(\"The mean R squared is : \",statistics.mean(r_square))\n",
    "\n",
    "        alpha = 0.95\n",
    "        p = ((1.0-alpha)/2.0) * 100\n",
    "        lower = max(0.0, numpy.percentile(r_square, p))\n",
    "        p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "        upper = min(1.0, numpy.percentile(r_square, p))\n",
    "        print('\\n%.1f R^2 confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))\n",
    "        histo = numpy.histogram(r_square)\n",
    "        _ = plt.hist(r_square, bins='auto')\n",
    "        plt.title(\"Histogram of R^2 {} using {} \".format(file, regression_model))\n",
    "        plt.xlim(0,1)\n",
    "        plt.xticks(np.arange(0, 1.1, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#overlapping_genes regression\n",
    "\n",
    "print(\"Training on all of the data and testing on all of the data\")\n",
    "#load all data\n",
    "import numpy\n",
    "import statistics\n",
    "#create individual datasets of each species\n",
    "for file in overlapping_genes:\n",
    "    for regression_model in regression_models:\n",
    "        raw_df = pd.read_csv(file)\n",
    "        #encode it to integers for use in the classification\n",
    "        raw_df_dropped = raw_df.drop(columns=['Individual',\"Species\",\"Age\",\"Group\"])\n",
    "        #set our Y predictor\n",
    "        ages = raw_df['Age'].astype(int)\n",
    "\n",
    "        # drop these columns from the dataframe\n",
    "\n",
    "\n",
    "        #define lists to hold our results from each algorithim\n",
    "        r_square = list()\n",
    "        median_score = list()\n",
    "        mean_score = list()\n",
    "        mean_squares = list()\n",
    "\n",
    "\n",
    "        # run bootstrap on all data and then tested on all data for the samples\n",
    "        n_iterations = 1000\n",
    "        for i in range(n_iterations):\n",
    "            print('\\r' + str(i),end = '')\n",
    "\n",
    "\n",
    "            #define the data with a new slice of the data\n",
    "            train_x, test_x, train_y, test_y = train_test_split(raw_df_dropped, ages, test_size=0.25, random_state=i)\n",
    "\n",
    "            model_for_use = regression_model\n",
    "            #fit the model on the training data\n",
    "            model_for_use.fit(train_x, train_y)\n",
    "            #predict the ages\n",
    "            predicted_ages = model_for_use.predict(test_x)\n",
    "            #append the scores to lists to report mean values\n",
    "            r_square.append(sklearn.metrics.r2_score(test_y, predicted_ages))\n",
    "            mean_squares.append(mean_squared_error(test_y, predicted_ages))\n",
    "            median_score.append(sklearn.metrics.median_absolute_error(test_y, predicted_ages))\n",
    "            mean_score.append(sklearn.metrics.mean_absolute_error(test_y, predicted_ages))\n",
    "\n",
    "        # #predict the ages\n",
    "        # predicted_ages = model_for_use.predict(test_x)\n",
    "        # a, b = best_fit(predicted_ages, test_y)\n",
    "        # yfit = [a + b * xi for xi in predicted_ages]\n",
    "        # matplotlib.pyplot.scatter(predicted_ages, test_y)\n",
    "        # matplotlib.pyplot.plot(predicted_ages, yfit)\n",
    "        # matplotlib.pyplot.xlabel(\"Predicted Ages in Years\")\n",
    "        # matplotlib.pyplot.ylabel(\"Actual Ages in Years\")\n",
    "        # matplotlib.pyplot.suptitle(\"Human Top 10 Genes Logistic Regression\", size=24)\n",
    "        \n",
    "        print(\"{} using {} \".format(file, regression_model))\n",
    "        print(\"\\nThe Average Mean absolute error is : \",statistics.mean(mean_squares))\n",
    "        print(\"The Average Median absolute error is : \",statistics.mean(median_score))\n",
    "        print(\"The median R squared is : \",statistics.median(r_square))\n",
    "        print(\"The mean R squared is : \",statistics.mean(r_square))\n",
    "\n",
    "        alpha = 0.95\n",
    "        p = ((1.0-alpha)/2.0) * 100\n",
    "        lower = max(0.0, numpy.percentile(r_square, p))\n",
    "        p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "        upper = min(1.0, numpy.percentile(r_square, p))\n",
    "        print('\\n%.1f R^2 confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))\n",
    "        histo = numpy.histogram(r_square)\n",
    "        _ = plt.hist(r_square, bins='auto')\n",
    "        plt.title(\"Histogram of R^2 {} using {} \".format(file, regression_model))\n",
    "        plt.xlim(0,1)\n",
    "        plt.xticks(np.arange(0, 1.1, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we run all of the classification models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
